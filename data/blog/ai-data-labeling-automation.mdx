---
title: "AI-Assisted Data Labeling and Curation"
date: "2025-09-16"
tags: ["ai", "data", "labeling", "quality"]
draft: true
summary: "Bootstrap labels, prioritize review with uncertainty, and keep datasets fresh."
---

Use model-in-the-loop to accelerate labeling and improve data quality.

## Active learning sketch

```python
scores = model.predict_proba(unlabeled)
uncertain = select_topk_by_entropy(scores, k=500)
labels = human_label(uncertain)
model.fit(merge(labeled, labels))
```

## Why this matters

- Better data beats bigger models for many tasks.

## How to use this today

- Start with weak supervision and active learning.
- Prioritize long-tail and high-impact slices.

## Common pitfalls

- Feedback not tracked; keep annotator metadata and quality checks.
- Stale datasets; schedule refreshes from production.

## What to try next

- Mine hard negatives from prod logs.
- Add data unit tests (schema, drift, leakage) in CI.
