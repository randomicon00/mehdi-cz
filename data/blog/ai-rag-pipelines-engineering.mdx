---
title: "Engineering Reliable RAG Pipelines"
date: "2025-09-16"
tags: ["ai", "rag", "retrieval", "evals"]
draft: true
summary: "Make retrieval-augmented generation predictable with chunking, reranking, caching, and evals."
---

Treat RAG as a system: indexing, retrieval, generation, and feedback loops.

## Runnable outline

```python
docs = chunk(load("/kb/*.md"), size=800, overlap=120)
index = embed_and_index(docs, model="text-embedding-3-large")
def answer(q):
  cands = index.similar(q, top_k=20)
  reranked = cross_encode(q, cands, model="bge-reranker-v2")[:6]
  return generate(q, reranked, guardrails=[citations, refusal_on_missing])
```

## Why this matters

- RAG shifts from hallucination to grounded answers.

## How to use this today

- Add reranking and caching; log retrieval quality per query.
- Evaluate with golden Q/A sets and slice metrics.

## Common pitfalls

- Over-chunking; hurts coherence and retrieval recall.
- No freshness strategy; implement delta indexing.

## What to try next

- Add query routing to structured data or calculators.
- Train domain rerankers with feedback.
